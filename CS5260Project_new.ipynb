{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19491,"status":"ok","timestamp":1648783196784,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"Bpy_eeoXVOYN","outputId":"79906a22-e445-4586-93be-315f9a9c15fd"},"outputs":[],"source":["# \"\"\" Mount Google Drive to Google Colab Notebook\n","# \"\"\" \n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# \"\"\" Change present working directory\n","# \"\"\"\n","# %cd /content/drive/MyDrive/CS5260/Github/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":509,"status":"ok","timestamp":1648783197291,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"aWC_C25NVpoQ","outputId":"82f91fb2-ccf8-483f-f700-ccaf5c262b4e"},"outputs":[],"source":["# %cd CS5260\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11069,"status":"ok","timestamp":1648783208359,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"dopRcy9uVrJk","outputId":"48d1de6a-a467-47df-faf8-10ec8dd7948d"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import skimage.io as io\n","import matplotlib.pyplot as plt\n","import torch.utils.data as data\n","import torch\n","import json\n","import os\n","import nltk\n","import random\n","from tqdm import tqdm\n","tqdm.pandas()\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1456,"status":"ok","timestamp":1648783209811,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"53x4XFyJsh50"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload\n","from src.utils.data_loader import get_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223},"executionInfo":{"elapsed":359,"status":"ok","timestamp":1648783210169,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"YiUKbENkV1Wu","outputId":"41ae1ef7-a24a-48cf-8a47-be61e7535c8d"},"outputs":[],"source":["with open('./data/vaild_dataset.json', 'r') as outfile:\n","  product_info = pd.read_json(json.load(outfile), orient=\"records\")\n","print(product_info.shape)\n","product_info.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1648783210170,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"pMygHtuZV9O2","outputId":"3afe04da-f2b7-4480-903b-c1ecbc86df4a"},"outputs":[],"source":["product_info[\"perCategory\"].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1648783210171,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"gPrbmFTGo29j"},"outputs":[],"source":["import re\n","def get_space_len(s):\n","  return len(max(re.findall(' +', s), key=len, default=[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1648783864090,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"ReMKWOZJ0wnE","outputId":"57c8c910-940d-481b-be0a-020291d3612e"},"outputs":[],"source":["product_info = product_info[product_info.valid == True]\n","product_info = product_info[product_info[\"perCategory\"].isin([\"AMAZON_FASHION\", 'All_Beauty',\n","                                                               'Toys_and_Games','Office_Products',\n","                                                              'Home_and_Kitchen','Electronics',\n","                                                              'Clothing_Shoes_and_Jewelry'])]\n","product_info[\"file_name\"] = product_info[\"imageURLHighRes\"].str.split(\"/\").str[-1]\n","\n","product_info = product_info.explode(\"description\")\n","product_info[\"description\"] = product_info[\"description\"].str.split(\".\")\n","product_info = product_info.explode(\"description\")\n","product_info = product_info.where(\n","    (product_info[\"description\"].str.len()>5) & \n","    (product_info[\"description\"].astype(str).apply(get_space_len)<4)).dropna().reset_index(drop=True)\n","product_info['sentence_index'] = product_info.groupby('file_name').cumcount()  # Enumerate Groups\n","product_info = product_info[product_info['sentence_index'] <= product_info[\"description\"].str.len().min()].reset_index(drop=True)\n","print(product_info.shape)\n","product_info.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":274},"executionInfo":{"elapsed":902,"status":"ok","timestamp":1648783868843,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"ZCb7qEoaV_N5","outputId":"3b75e6c9-74f7-481b-f8b0-77ae7613a214"},"outputs":[],"source":["idex = random.randint(0, product_info.shape[0])\n","img_url = product_info.loc[idex,\"imageURLHighRes\"]\n","caption = product_info.loc[idex,\"description\"]\n","Category = product_info.loc[idex,\"perCategory\"]\n","print(caption)\n","print(img_url)\n","print(Category)\n","I = io.imread(img_url)\n","plt.axis('off')\n","plt.imshow(I)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":392,"status":"ok","timestamp":1648783871639,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"2lWsRjrKWA-5","outputId":"71404431-a1db-49cb-d425-927c710f7e05"},"outputs":[],"source":["from torchvision import transforms\n","from sklearn.preprocessing import OneHotEncoder\n","\n","image_dict = product_info[\"imageURLHighRes\"].to_dict()\n","caption_dict = product_info[\"description\"].to_dict()\n","category_dict = product_info[\"perCategory\"].to_dict()\n","onehot_cat = OneHotEncoder().fit_transform(np.array([*category_dict.values()], dtype=object).reshape(-1, 1)).toarray()\n","sentence_id_dict = product_info[\"sentence_index\"].to_dict()\n","onehot_seq = OneHotEncoder().fit_transform(np.array([*sentence_id_dict.values()], dtype=object).reshape(-1, 1)).toarray()\n","one_hot = np.concatenate((onehot_cat, onehot_seq), axis=1)\n","print(one_hot.shape)\n","\n","# Define a transform to pre-process the training images.\n","transform_train = transforms.Compose([ \n","    transforms.Resize(500),                          # smaller edge of image resized to 256\n","    transforms.Resize((320,320)),                      # get 224x224 crop from random location\n","    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n","])\n","\n","# Set the minimum word count threshold.\n","vocab_threshold = 5\n","\n","mode = \"train\"\n","\n","# Specify the batch size.\n","# we will pass 10 images at a time. So, m = 10\n","batch_size = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1648783873166,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"uhsjK7Yiudqs","outputId":"ea2064a8-1576-43f7-8b22-ee07d1d629ef"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload\n","from src.utils.data_loader import get_loader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3529,"status":"ok","timestamp":1648783878236,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"J7dFL3GaWc7u","outputId":"023db086-e529-4f4f-cd35-d3486f01872f"},"outputs":[],"source":["data_loader = get_loader(product_info,\n","                        transform=transform_train,\n","                        mode='train',\n","                        image_type=\"imageURLHighRes\",\n","                        caption_type=\"description\",\n","                        batch_size=batch_size,\n","                        vocab_threshold=vocab_threshold,\n","                        vocab_from_file=False)\n","\n","print('The shape of first image:', data_loader.dataset[0][0].shape)\n","print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))\n","\n","# Randomly sample a caption length, and sample indices with that length.\n","indices = data_loader.dataset.get_indices()\n","print('sampled indices:', indices)\n","\n","# Create and assign a batch sampler to retrieve a batch with the sampled indices.\n","new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n","data_loader.batch_sampler.sampler = new_sampler\n","    \n","# Obtain the batch.\n","images, onehot_cat, onehot_enc_seq, captions = next(iter(data_loader))\n","    \n","print('images.shape:', images.shape)\n","print('onehot_cat.shape:', onehot_cat.shape)\n","print('captions.shape:', captions.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"elapsed":954,"status":"ok","timestamp":1648783880398,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"osvFMe8pbEpB","outputId":"ef534361-169e-484f-f6a6-c680cc7781d2"},"outputs":[],"source":["plt.axis('off')\n","plt.imshow(images[0].detach().T)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"CfFzanMoZ0TF"},"source":["# Training Setup "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1648783882777,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"amKH_lyVZtwf","outputId":"e79a7a68-f084-4ca4-ebd7-b1fc9fc84e6a"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","import numpy as np\n","import sys\n","import os\n","import math\n","from sklearn.model_selection import train_test_split\n","import torch.utils.data as data\n","import nltk\n","from nltk.translate.bleu_score import corpus_bleu\n","nltk.download('punkt')\n","\n","%load_ext autoreload\n","%autoreload\n","from src.utils.data_loader import get_loader\n","from src.models.CNN_Encoder import EncoderCNN\n","from src.models.RNN_Decoder import DecoderRNN\n","from src.models.MLP_Encoder import MlpEncoder\n","from src.utils.utils_trainer import get_batch_caps, get_hypothesis, adjust_learning_rate"]},{"cell_type":"markdown","metadata":{"id":"t8K2VoUxdaaM"},"source":["# Training Model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1648783885282,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"1rPrzyGjdYGU"},"outputs":[],"source":["def train(epoch, \n","          encoder,\n","          decoder,\n","          encoder_optimizer,\n","          decoder_optimizer,\n","          criterion, \n","          total_step, \n","          num_epochs, \n","          data_loader, \n","          write_file, \n","          save_every = 1):\n","    \"\"\" Train function for a single epoch. \n","    Arguments: \n","    ----------\n","    - epoch - number of current epoch\n","    - encoder - model's Encoder\n","    - decoder - model's Decoder\n","    - optimizer - model's optimizer (Adam in our case)\n","    - criterion - loss function to optimize\n","    - num_epochs - total number of epochs\n","    - data_loader - specified data loader (for training, validation or test)\n","    - write_file - file to write the training logs\n","    \n","    \"\"\"\n","    epoch_loss = 0.0\n","    epoch_perplex = 0.0\n","    \n","    for i_step in range(1, total_step+1):\n","        # training mode on\n","        encoder.train() # no fine-tuning for Encoder\n","        # mlp.train()\n","        decoder.train()\n","        \n","        # Randomly sample a caption length, and sample indices with that length.\n","        indices = data_loader.dataset.get_indices()\n","        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n","        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n","        data_loader.batch_sampler.sampler = new_sampler\n","        \n","        # Obtain the batch.\n","        images, onehot_cat, onehot_seq, captions = next(iter(data_loader))\n","        # target captions, excluding the first word\n","        captions_target = captions[:, 1:].to(device) \n","        # captions for training without the last word\n","        captions_train = captions[:, :-1].to(device)\n","\n","        # Move batch of images and captions to GPU if CUDA is available.\n","        images = images.to(device)\n","\n","        onehot_cat = torch.cat((onehot_cat.view(batch_size,-1), \n","                                onehot_seq.view(batch_size,-1)), 1).type('torch.FloatTensor').view(batch_size,-1).to(device)\n","        # Zero the gradients.\n","        decoder.zero_grad()\n","        # mlp.zero_grad()\n","        encoder.zero_grad()\n","        \n","        # Pass the inputs through the CNN-RNN model.\n","        features = encoder(images)\n","\n","        outputs, atten_weights = decoder(captions= captions_train,\n","                                         features = features,\n","                                         category_enc = onehot_cat)\n","        \n","        # Calculate the batch loss.\n","        loss = criterion(outputs.view(-1, vocab_size), captions_target.reshape(-1))\n","        \n","        # Backward pass.\n","        loss.backward()\n","        \n","        # Update the parameters in the optimizer.\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","        \n","        perplex = np.exp(loss.item())\n","        epoch_loss += loss.item()\n","        epoch_perplex += perplex\n","        \n","        stats = 'Epoch train: [%d/%d], Step train: [%d/%d], Loss train: %.4f, Perplexity train: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), perplex)\n","        \n","        \n","        # Print training statistics (on same line).\n","        print('\\r' + stats, end=\"\")\n","        sys.stdout.flush()\n","        \n","        # Print training statistics to file.\n","        write_file.write(stats + '\\n')\n","        write_file.flush()\n","        \n","        # Print training statistics (on different line).\n","        if i_step % print_every == 0:\n","            print('\\r' + stats)\n","        \n","    epoch_loss_avg = epoch_loss / total_step\n","    epoch_perp_avg = epoch_perplex / total_step\n","    \n","    print('\\r')\n","    print('Epoch train:', epoch)\n","    print('\\r' + 'Avg. Loss train: %.4f, Avg. Perplexity train: %5.4f' % (epoch_loss_avg, epoch_perp_avg), end=\"\")\n","    print('\\r')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1648783886814,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"ZH0WCQWAdZrQ"},"outputs":[],"source":["def validate(epoch, \n","             encoder, \n","             decoder, \n","             encoder_optimizer, \n","             decoder_optimizer, \n","             criterion, \n","             total_step, num_epochs, data_loader, write_file, bleu_score_file):\n","    \"\"\" Validation function for a single epoch. \n","    Arguments: \n","    ----------\n","    - epoch - number of current epoch\n","    - encoder - model's Encoder (evaluation)\n","    - decoder - model's Decoder (evaluation)\n","    - optimizer - model's optimizer (Adam in our case)\n","    - criterion - optimized loss function\n","    - num_epochs - total number of epochs\n","    - data_loader - specified data loader (for training, validation or test)\n","    - write_file - file to write the validation logs\n","    \"\"\"\n","    epoch_loss = 0.0\n","    epoch_perplex = 0.0\n","    references = []\n","    hypothesis = []\n","      \n","    for i_step in range(1, total_step+1):\n","        # evaluation of encoder and decoder\n","        encoder.eval()\n","        decoder.eval()\n","        val_images, val_onehot_cat, val_onehot_seq, val_captions, caps_all = next(iter(data_loader))\n","        \n","        val_captions_target = val_captions[:, 1:].to(device) \n","        val_captions = val_captions[:, :-1].to(device)\n","        val_images = val_images.to(device)\n","       \n","        val_onehot_cat = torch.cat((val_onehot_cat.view(batch_size,-1), \n","                                    val_onehot_seq.view(batch_size,-1)), 1).type('torch.FloatTensor').view(batch_size,-1).to(device)\n","        \n","        features_val = encoder(val_images)\n","        outputs_val, atten_weights_val = decoder(captions= val_captions,\n","                                                 features = features_val,\n","                                                 category_enc=val_onehot_cat)\n","        loss_val = criterion(outputs_val.view(-1, vocab_size), \n","                             val_captions_target.reshape(-1))\n","        \n","        # preprocess captions and add them to the list\n","        caps_processed = get_batch_caps(caps_all, batch_size=batch_size)\n","        references.append(caps_processed)\n","        # get corresponding indicies from predictions\n","        # and form hypothesis from output\n","        terms_idx = torch.max(outputs_val, dim=2)[1]\n","        hyp_list = get_hypothesis(terms_idx, data_loader=data_loader)\n","        hypothesis.append(hyp_list)\n","        \n","        perplex = np.exp(loss_val.item())\n","        epoch_loss += loss_val.item()\n","        epoch_perplex += perplex\n","        \n","        stats = 'Epoch valid: [%d/%d], Step valid: [%d/%d], Loss valid: %.4f, Perplexity valid: %5.4f' % (epoch, num_epochs, i_step, total_step, loss_val.item(), perplex)\n","        \n","        # Print training statistics (on same line).\n","        print('\\r' + stats, end=\"\")\n","        sys.stdout.flush()\n","        \n","        # Print training statistics to file.\n","        write_file.write(stats + '\\n')\n","        write_file.flush()\n","        \n","        # Print training statistics (on different line).\n","        if i_step % print_every == 0:\n","            print('\\r' + stats)\n","    \n","    epoch_loss_avg = epoch_loss / total_step\n","    epoch_perp_avg = epoch_perplex / total_step\n","            \n","    # prepare the proper shape for computing BLEU scores\n","    references = np.array(references).reshape(total_step*batch_size, -1)\n","    #hyps = np.array(hypothesis).reshape(total_step*batch_size, -1)\n","    hyps = np.concatenate(np.array(hypothesis))\n","        \n","    bleu_1 = corpus_bleu(references, hyps, weights = (1.0, 0, 0, 0))\n","    bleu_2 = corpus_bleu(references, hyps, weights = (0.5, 0.5, 0, 0))\n","    bleu_3 = corpus_bleu(references, hyps, weights = (1.0/3.0, 1.0/3.0, 1.0/3.0, 0))\n","    bleu_4 = corpus_bleu(references, hyps, weights = (0.25, 0.25, 0.25, 0.25))\n","    # append individual n_gram scores\n","    #bleu_score_list.append((bleu_1, bleu_2, bleu_3, bleu_4))\n","    \n","    print('\\r')\n","    print('Epoch valid:', epoch)\n","    epoch_stat = 'Avg. Loss valid: %.4f, Avg. Perplexity valid: %5.4f, \\\n","    BLEU-1: %.2f, BLEU-2: %.2f, BLEU-3: %.2f, BLEU-4: %.2f' % (epoch_loss_avg, epoch_perp_avg, bleu_1, bleu_2, bleu_3, bleu_4)\n","    \n","    print('\\r' + epoch_stat, end=\"\")\n","    print('\\r')\n","    \n","    bleu_score_file.write(epoch_stat + '\\n')\n","    bleu_score_file.flush()\n","    return bleu_1, bleu_2, bleu_3, bleu_4"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1648783887816,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"x9E6y2t-d43c"},"outputs":[],"source":["batch_size = 5          # batch size, change to 64\n","vocab_threshold = 5        # minimum word count threshold\n","vocab_from_file = True    # if True, load existing vocab file\n","embed_size = 125           # dimensionality of image and word embeddings\n","hidden_size = 512          # number of features in hidden state of the RNN decoder\n","num_features = 2048        # number of feature maps, produced by Encoder\n","num_epochs = 200               # number of training epochs\n","save_every = 1             # determines frequency of saving model weights\n","print_every = 100          # determines window for printing average loss\n","\n","log_train = './logs/training_log.txt'       # name of files with saved training loss and perplexity\n","log_val = './logs/validation_log.txt'\n","bleu = './logs/bleu.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1810,"status":"ok","timestamp":1648783890915,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"AAy3gNw9Z8B-","outputId":"3fd43465-1a24-48cc-b374-5078b1952a2c"},"outputs":[],"source":["train_df, val_test_df = train_test_split(product_info, test_size=0.4)\n","train_df.reset_index(drop=True,inplace=True)\n","valid_df, test_df = train_test_split(val_test_df, test_size=0.5)\n","valid_df.reset_index(drop=True,inplace=True)\n","test_df.reset_index(drop=True,inplace=True)\n","\n","\n","# transform_train = transforms.Compose([ \n","#     transforms.Resize(500),                          # smaller edge of image resized to 256\n","#     transforms.Resize((320,320)),                      # get 224x224 crop from random location\n","#     transforms.ToTensor(),                           # convert the PIL Image to a tensor\n","#     transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n","#                          (0.229, 0.224, 0.225))])\n","transform_train = transforms.Compose([ \n","    transforms.Resize(500),                          # smaller edge of image resized to 256\n","    transforms.Resize((320,320)),                      # get 224x224 crop from random location\n","    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n","])\n","\n","# Build data loader.\n","data_loader = get_loader(train_df,\n","                         transform=transform_train,\n","                         mode='train',\n","                         image_type=\"imageURLHighRes\",\n","                         caption_type=\"description\",\n","                         batch_size=batch_size,\n","                         vocab_threshold=vocab_threshold,\n","                         vocab_from_file=False)\n","\n","# The size of the vocabulary.\n","vocab_size = len(data_loader.dataset.vocab)\n","\n","# Setup the transforms\n","transform_test = transforms.Compose([ \n","    transforms.Resize(500),                          # smaller edge of image resized to 256\n","    transforms.Resize((320,320)),                      # get 224x224 crop from random location\n","    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n","])\n","\n","# Create the data loader.\n","\n","valid_data_loader = get_loader(valid_df,\n","                               transform=transform_test,\n","                               mode='valid',\n","                               image_type=\"imageURLHighRes\",\n","                               caption_type=\"description\",\n","                               batch_size=batch_size)\n","\n","\n","total_step_valid = math.ceil(len(valid_data_loader.dataset.caption_lengths) / valid_data_loader.batch_sampler.batch_size)\n","total_step_valid"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1153,"status":"ok","timestamp":1648783893854,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"aPmdPeJafaWF","outputId":"b1db6f27-a465-4407-c780-2224e415619d"},"outputs":[],"source":["# Initialize the encoder and decoder. \n","encoder = EncoderCNN()\n","decoder = DecoderRNN(num_features = num_features, \n","                     embedding_dim = embed_size, \n","                     category_dim = len(set(category_dict.values())) + len(set(sentence_id_dict.values())) ,\n","                     hidden_dim = hidden_size, \n","                     vocab_size = vocab_size)\n","\n","\n","# Move models to GPU if CUDA is available. \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","encoder.to(device)\n","decoder.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1648783893855,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"CiCEogngffwY","outputId":"3b33f542-965e-455c-a859-4e7b63fa205b"},"outputs":[],"source":["print(device)\n","# Define the loss function. \n","criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n","\n","#params = list(decoder.parameters()) + list(encoder.parameters()) \n","params = list(decoder.parameters())\n","\n","# TODO #4: Define the optimizer.\n","grad_clip = 5.  # clip gradients at an absolute value of\n","alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n","\n","encoder_lr = 1e-3  # learning rate for encoder if fine-tuning\n","encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),lr=encoder_lr)\n","encoder_lr_scheduler = torch.optim.lr_scheduler.CyclicLR(encoder_optimizer, \n","                                                         base_lr=encoder_lr/10, max_lr=encoder_lr*10, \n","                                                         step_size_up = 10 , step_size_down=20, cycle_momentum=False)\n","\n","\n","decoder_lr = 1e-3  # learning rate for decoder\n","decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),lr=decoder_lr)\n","decoder_lr_scheduler = torch.optim.lr_scheduler.CyclicLR(decoder_optimizer, \n","                                                         base_lr=decoder_lr/10, max_lr=decoder_lr*10, \n","                                                         step_size_up = 10 , step_size_down=20, cycle_momentum=False)\n","\n","fine_tune_encoder = False  # fine-tune encoder?\n","checkpoint = None  # path to checkpoint, None if none\n","\n","# Set the total number of training steps per epoch.\n","total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R0ygLO8gdljX","outputId":"86fb3391-5c79-4571-8982-c8859f0cda55"},"outputs":[],"source":["from datetime import datetime\n","now = datetime.now()\n","current_time = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","if not os.path.exists(f'../models_new/{current_time}'):\n","  os.makedirs(f'../models_new/{current_time}')\n","\n","# Open the training log file.\n","file_train = open(log_train, 'w')\n","file_val = open(log_val, 'w')\n","bleu_score_file = open(bleu, 'w')\n","\n","# store BLEU scores in list \n","bleu_scores = []\n","total_step_valid = math.ceil(len(valid_data_loader.dataset.caption_lengths) / valid_data_loader.batch_sampler.batch_size)\n","epochs_since_improvement = 0\n","best_bleu = 0.\n","\n","for epoch in range(0, num_epochs+1):\n","    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n","    if epochs_since_improvement == 50:\n","        break\n","    if epochs_since_improvement > 0 and epochs_since_improvement % 20 == 0:\n","        adjust_learning_rate(decoder_optimizer, 0.8)\n","        if fine_tune_encoder:\n","            adjust_learning_rate(encoder_optimizer, 0.8)\n","\n","    train(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, total_step, num_epochs =num_epochs,\n","          data_loader = data_loader,\n","          write_file = file_train, \n","          save_every = 1)\n","    \n","    bleus= validate(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, \n","                                             total_step = total_step_valid, \n","                                             num_epochs = num_epochs, \n","                                             data_loader = valid_data_loader, write_file=file_val, bleu_score_file=bleu_score_file)\n","    \n","    # encoder_lr_scheduler.step()\n","    # decoder_lr_scheduler.step()\n","    \n","    # Check if there was an improvement\n","    is_best = sum(bleus)/4 > best_bleu\n","    best_bleu = max(sum(bleus)/4, best_bleu)\n","    if not is_best:\n","        epochs_since_improvement += 1\n","        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n","    else:\n","        torch.save(decoder.state_dict(), os.path.join(f'../models_new/{current_time}', 'decoder-%d.pkl' % epoch))\n","        torch.save(encoder.state_dict(), os.path.join(f'../models_new/{current_time}', 'encoder-%d.pkl' % epoch))\n","        epochs_since_improvement = 0\n","    \n","file_train.close()\n","file_val.close()\n","bleu_score_file.close()"]},{"cell_type":"markdown","metadata":{"id":"EXZjXmeKkVpF"},"source":["# Generate Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1648783226919,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"eoKMDZj9fSHB"},"outputs":[],"source":["# Setup the transforms\n","# transform_test = transforms.Compose([ \n","#     transforms.Resize((320,320)),                          # smaller edge of image resized to 256\n","#     transforms.ToTensor(),                           # convert the PIL Image to a tensor\n","#     transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n","#                          (0.229, 0.224, 0.225))])\n","\n","transform_test = transforms.Compose([ \n","    transforms.Resize(500),                          # smaller edge of image resized to 256\n","    transforms.Resize((320,320)),                      # get 224x224 crop from random location\n","    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1648783226920,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"r2OmJZeAkZOA","outputId":"5f64ee9e-32ad-4ec6-90a2-660f18d274f7"},"outputs":[],"source":["test_data_loader = get_loader(test_df,\n","                              image_type=\"imageURLHighRes\",\n","                              caption_type=\"description\",\n","                              transform=transform_test,\n","                              batch_size=1,\n","                              mode='test')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2162,"status":"ok","timestamp":1648783480190,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"gG0n30hxkZ1p","outputId":"4ca52eba-f618-4b93-b555-408ec974c7ba"},"outputs":[],"source":["encoder_file = 'encoder-4.pkl' \n","decoder_file = 'decoder-4.pkl'\n","\n","embed_size = 125           # dimensionality of image and word embeddings\n","hidden_size = 512          # number of features in hidden state of the RNN decoder\n","num_features = 2048        # number of feature maps, produced by Encoder\n","\n","# The size of the vocabulary.\n","vocab_size = 3633#len(test_data_loader.dataset.vocab)\n","\n","# Initialize the encoder and decoder, and set each to inference mode.\n","encoder = EncoderCNN()\n","encoder.eval()\n","decoder = DecoderRNN(num_features = num_features, \n","                     embedding_dim = embed_size,\n","                     category_dim = len(set(category_dict.values())) + len(set(sentence_id_dict.values())),\n","                     hidden_dim = hidden_size, \n","                     vocab_size = vocab_size)\n","decoder.eval()\n","\n","current_time = \"2022-03-31-16-43-01\"\n","# Load the trained weights.\n","encoder.load_state_dict(torch.load(os.path.join(f'../models_new/{current_time}', encoder_file), map_location='cpu'))\n","decoder.load_state_dict(torch.load(os.path.join(f'../models_new/{current_time}', decoder_file), map_location='cpu'))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Move models to GPU if CUDA is available.\n","encoder.to(device)\n","decoder.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1648783779953,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"V0KU8OfClOIW"},"outputs":[],"source":["def clean_sentence(output, data_loader):\n","    vocab = data_loader.dataset.vocab.idx2word\n","    words = [vocab.get(idx) for idx in output]\n","    words = [word for word in words if word not in (',', '.', '<end>')]\n","    sentence = \" \".join(words)\n","    \n","    return sentence\n","\n","def get_prediction(data_loader):\n","    orig_image, image, cat_onehot, seq_onehot, caption= next(iter(data_loader))\n","    plt.imshow(orig_image.squeeze())\n","    print(cat_onehot.size())\n","    plt.title(caption)\n","    plt.show()\n","    image = image.to(device)\n","    onehot = torch.cat((cat_onehot.view(1,-1), \n","                            seq_onehot.view(1,-1)), 1).type('torch.FloatTensor').view(1,-1).to(device)\n","    \n","    features = encoder(image)\n","    output, atten_weights = decoder.greedy_search(features, onehot)    \n","    sentence = clean_sentence(output,data_loader)\n","    print(sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316},"executionInfo":{"elapsed":1676,"status":"ok","timestamp":1648783799998,"user":{"displayName":"Ma Yuan","userId":"07887771367764948388"},"user_tz":-480},"id":"Yp_rp5M2lLWR","outputId":"1407852a-bb4c-4af6-ac71-01aff83b7d2f"},"outputs":[],"source":["get_prediction(test_data_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1y3w5XZ2lPLW"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPYOOfPH6GRnxltdA0RDJZ1","collapsed_sections":[],"name":" CS5260Project_new.ipynb","provenance":[{"file_id":"1YM0Jy9EduI_QyNiWzVfvTofHNsteVu81","timestamp":1648551625627}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":0}
